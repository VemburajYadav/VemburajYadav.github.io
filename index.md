---
layout: plain 
name: index
title: 
description: >
  My Personal Homepage.
hide_description: true
cover: true
---

<h2 class="h1 index-header" id="about">About Me </h2>


I am a Researcher at the [Augmented Vision](https://av.dfki.de/) department of [German Research Center for Artificial Intelligence](https://www.dfki.de/web) (DFKI GmbH), working under the supervision of Dr. [Alain Pagani](https://av.dfki.de/members/pagani/) and Prof. [Didier Stricker](https://av.dfki.de/members/stricker/).  My research focuses on core challenges in computer vision, with a special emphasis on inferring 3D information from 2D data‚Äîparticularly in the areas of generalizable 3D reconstruction, optical flow estimation, and novel view synthesis. In parallel, I contribute to the development of AR/VR applications, with a focus on interactive and immersive data visualization. My work aims to bridge foundational vision research with user-centered extended reality systems, enabling practical applications in augmented environments.

I earned my master's degree in [Computational Sciences in Engineering (CSE)](https://www.tu-braunschweig.de/en/cse) from [TU Braunschweig](https://www.tu-braunschweig.de/), Germany. For my master‚Äôs thesis, I worked under the supervision of Prof. [Nils Thuerey](https://ge.in.tum.de/about/n-thuerey/) at [TU Munich (TUM)](https://www.tum.de/) on developing efficient learning-based methods for particle simulations to generate physically accurate fluid animations for computer graphics.

During my studies, I gained practical experience through research internships at [Volkswagen AG](https://www.volkswagen-nutzfahrzeuge.de/de/elektrisch-und-autonom/autonomes-fahren.html) and the [German Aerospace Center (DLR)](https://www.dlr.de/de/ts/forschung-und-transfer/themen/verkehrssystem), where I worked on advanced perception algorithms for autonomous driving and user-focused driver-assistance systems.

My academic journey began with a Bachelor's degree in Mechanical Engineering from [Veermata Jijabai Technological Institute (VJTI)](https://vjti.ac.in/), India.


<div class="body-social sidebar-social">
  <ul>
    <li> <a href="mailto:vemburajyadav1994@gmail.com" title="vemburajyadav1994@gmail.com" class="no-mark-external" target="_blank"> <span class="icon-mail"></span> <span aria-hidden="true">Email </span><span class="sr-only">Donny's Email Address</span></a></li>
    <!-- <li> <a href="https://scholar.google.com/citations?user=GqgGZlQAAAAJ" title="Google Scholar" class="no-mark-external" target="_blank"> <span class="icon-googlescholar"></span> <span aria-hidden="true">Google Scholar </span><span class="sr-only">Donny's Google Scholar profile</span></a></li> -->
    <li> <a href="https://github.com/VemburajYadav" title="GitHub" class="no-mark-external" target="_blank"> <span class="icon-github"></span> <span aria-hidden="true">GitHub </span><span class="sr-only">Donny's Github</span></a></li>
    <li> <a href="https://www.linkedin.com/in/vemburaj-yadav" title="LinkedIn" class="no-mark-external" target="_blank"> <span class="icon-linkedin2"></span> <span aria-hidden="true">LinkedIn </span><span class="sr-only">Donny's LinkedIn</span></a></li>
    <li> <a href="https://twitter.com/yadav_vemburaj" title="Twitter" class="no-mark-external" target="_blank"> <span><svg xmlns="http://www.w3.org/2000/svg" width="15.2" height="15.5" fill="currentColor" class="bi bi-twitter-x" viewBox="0 0 16 16"><path d="M12.6.75h2.454l-5.36 6.142L16 15.25h-4.937l-3.867-5.07-4.425 5.07H.316l5.733-6.57L0 .75h5.063l3.495 4.633L12.601.75Zm-.86 13.028h1.36L4.323 2.145H2.865z"/></svg></span> <span aria-hidden="true">Twitter </span><span class="sr-only">Donny's Twitter</span></a></li>
  </ul>
</div>

---

<h2 class="h1 index-header" id="publications">Selected Publications </h2>
<!-- <p class='hl-sen'> ü§ñüß†üëåüèº He prefers simple yet effective solutions </p> -->

<div class="pub">
  <div class="pub-title">OptSplat: Recurrent Optimization for Generalizable Reconstruction and Novel View Renderings</div>
  <div class="pub-venue">Under Review</div>
  <div class="pub-authors">Vemburaj Yadav, Alain Pagani and Didier Stricker</div>
  <div>[<a href="https://www.dropbox.com/scl/fi/wyv27h9uinfivfh4gxzu7/OptSplat.pdf?rlkey=cqc3zd55jyc9laz4y45zfd91g&st=vw71fezc&dl=0">paper</a>] [<a href="https://github.com/VemburajYadav/OptSplat">code</a>] [<a href="https://vemburajyadav.github.io/OptSplat">project page</a>] </div>
  <!-- <div> TL;DR: MVSplat360 is a feed‚Äëforward model that combines 3DGS with SVD to achieve 360¬∞ NVS for complex scenes with less than 5 input views. </div>  -->
</div>

<div class="pub">
  <div class="pub-title">MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images</div>
  <div class="pub-venue">ECCV 2024 (Oral)</div>
  <div class="pub-authors">Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai</div>
  <div>[<a href="https://arxiv.org/abs/2403.14627">arXiv</a>] [<a href="https://github.com/donydchen/mvsplat">code</a>] [<a href="https://donydchen.github.io/mvsplat">project
    page</a>] </div>
</div>

<div class="pub">
  <div class="pub-title">MuRF: Multi-Baseline Radiance Fields</div>
  <div class="pub-venue">CVPR 2024</div>
  <div class="pub-authors">Haofei Xu, Anpei Chen, Yuedong Chen, Christos Sakaridis, Yulun Zhang, Marc Pollefeys, Andreas Geiger, <i>et al.</i></div>
  <div>[<a href="https://arxiv.org/abs/2312.04565">arXiv</a>] [<a href="https://github.com/autonomousvision/murf">code</a>] [<a href="https://haofeixu.github.io/murf/">project
    page</a>] </div>
</div>

<div class="pub">
  <div class="pub-title">Explicit Correspondence Matching for Generalizable Neural Radiance Fields</div>
  <div class="pub-venue">arXiv 2023</div>
  <div class="pub-authors">Yuedong Chen, Haofei Xu, Qianyi Wu, Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai</div>
  <div>[<a href="http://arxiv.org/abs/2304.12294">arXiv</a>] [<a href="https://github.com/donydchen/matchnerf">code</a>] [<a href="https://donydchen.github.io/matchnerf/">project
    page</a>] </div>
</div>

<div class="pub">
  <div class="pub-title">Sem2NeRF: Converting Single-View Semantic Masks to Neural Radiance Fields</div>
  <div class="pub-venue">ECCV 2022</div>
  <div class="pub-authors">Yuedong Chen, Qianyi Wu, Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai</div>
  <div>[<a href="https://arxiv.org/abs/2203.10821">arXiv</a>] [<a href="https://github.com/donydchen/sem2nerf">code</a>] [<a href="https://donydchen.github.io/sem2nerf/">project
    page</a>] [<a href="https://www.youtube.com/watch?v=cYr3Dz8N_9E">demo video</a>] </div>
</div>

<div class="pub">
  <div class="pub-title">Object-Compositional Neural Implicit Surfaces</div>
  <div class="pub-venue">ECCV 2022</div>
  <div class="pub-authors">Qianyi Wu, Xian Liu, Yuedong Chen, Kejie Li, Chuanxia Zheng, Jianfei Cai, and Jianmin Zheng</div>
  <div>[<a href="https://arxiv.org/abs/2207.09686">arXiv</a>] [<a href="https://github.com/QianyiWu/objsdf">code</a>]
    [<a href="https://wuqianyi.top/objectsdf/">project
      page</a>] [<a href="https://youtu.be/23vxOV19bEw">demo video</a>] </div>
</div>

<div class="pub">
  <div class="pub-title">Towards Unbiased Visual Emotion Recognition via Causal Intervention</div>
  <div class="pub-venue">ACM Multimedia 2022</div>
  <div class="pub-authors">Yuedong Chen, Xu Yang, Tat-Jen Cham, and Jianfei Cai</div>
  <div>[<a href="https://arxiv.org/abs/2107.12096">arXiv</a>] [<a href="https://github.com/donydchen/causal_emotion">code</a>]
  </div>
</div>

<div class="pub">
  <div class="pub-title">GeoConv: Geodesic Guided Convolution for Facial Action Unit Recognition</div>
  <div class="pub-venue">Pattern Recognition 2022</div>
  <div class="pub-authors">Yuedong Chen, Guoxian Song, Zhiwen Shao, Jianfei Cai, Tat-Jen Cham, and Jianmin Zheng</div>
  <div>[<a href="https://arxiv.org/abs/2003.03055">arXiv</a>] 
    <!-- [<a href="#">code (coming soon)</a>] -->
  </div>
</div>

<div class="pub">
  <div class="pub-title">Facial Motion Prior Networks for Facial Expression Recognition</div>
  <div class="pub-venue">VCIP 2019 (Oral)</div>
  <div class="pub-authors">Yuedong Chen, Jianfeng Wang, Shikai Chen, Zhongchao Shi, and Jianfei Cai</div>
  <div>[<a href="https://arxiv.org/abs/1902.08788">arXiv</a>] [<a href="https://github.com/donydchen/FMPN-FER">code</a>]</div>
</div>

<div>
  <br>
  More on <a href="https://scholar.google.com.sg/citations?user=GqgGZlQAAAAJ&hl=en" target="_blank">Google Scholar</a>
</div>

---

<h2 class="h1 index-header" id="projects">Projects & Talks</h2>

<div style="padding-top: 20px; margin-bottom: -15px">
  <ul>
    <li>28-01-2025, Invited talk "Feed-forward NVS from Sparse Inputs‚Äã" at <i>Amazon, Tel Aviv</i>, hosted by <a href="https://www.linkedin.com/in/lior-fritz-6457a796">Lior Fritz</a>.</li>
    <li>08-11-2024, Invited talk "Feed-forward Novel View Synthesis" at <i>Wayve, London</i>, hosted by <a href="https://www.linkedin.com/in/joepolin">Joe Polin</a>.</li>
  </ul>
</div>

<div class="demo-proj-row">

  <div class="card">
    <a href="https://bsky.app/profile/donydchen.bsky.social/post/3lldtgeyitc2h" target="_blank">
      <div><img src="assets/img/talk_3dv25.png" /></div>
    </a>
    <div class="cdesc">Invited talk at <a href="https://3dvconf.github.io/2025/call-for-nectar-track" target="_blank">3DV25 Nectar Track</a></div>
  </div>

  <div class="card">
    <a href="https://ajcai2024.org/files/AJCAI_Booklet.pdf#page=15.09" target="_blank">
      <div><img src="assets/img/talk_ajcai2024.png" /></div>
    </a>
    <div class="cdesc">Invited talk at <a href="https://ajcai2024.org" target="_blank">AJCAI 2024</a></div>
  </div>

  <!-- <div class="card">
    <a href="https://wayve.ai" target="_blank">
      <div style="margin: 25px 0;"><img src="assets/img/talk_wayve_uk.png" /></div>
    </a>
    <div class="cdesc">Invited talk at WAYVE(London)</div>
  </div> -->

  <div class="card">
    <a href="https://eccv2024.ecva.net/virtual/2024/poster/1231" target="_blank">
      <div><img src="assets/img/eccv24_oral.jpeg" /></div>
    </a>
    <div class="cdesc">Oral presentation at ECCV 2024</div>
  </div>

  <div class="card">
    <a href="https://cepoca.cn/lectureHall/lectureRoomDetail/?liveUid=cd40fae9992618669ccf17e09efa7b76" target="_blank">
      <div><img src="assets/img/talk_szhy.jpeg" /></div>
    </a>
    <div class="cdesc">Invited talk at SHUZIHUANYU</div>
  </div>

  <div class="card">
    <a href="https://www.bilibili.com/video/BV1sAWhe1ENw/" target="_blank">
      <div><img src="assets/img/talk_3dcver.jpeg" /></div>
    </a>
    <div class="cdesc">Invited talk at 3DCVer</div>
  </div>  

  <div class="card">
    <a href="https://github.com/lyndonzheng/Pluralistic-Inpainting#gui" target="_blank">
      <div><img src="assets/img/openday22_demo.jpeg" /></div>
    </a>
    <div class="cdesc">Demo at Monash Uni. <a href="https://www.monash.edu/open-day">Open Day 2022</a></div>
  </div>
  
  <div class="card">
    <a href="https://github.com/donydchen/ganimation_replicate" target="_blank">
      <div><img src="assets/img/ganimation_proj.png" /></div>
    </a>
    <div class="cdesc">A popular replication of <a href="https://github.com/albertpumarola/GANimation">GANimation</a></div>
  </div>
</div>

---


<h2 class="h1 index-header" id="misc">Miscellanies</h2>

<div style="margin-bottom: 40px;">
  <ul>
    <li>Conference Reviewer: ECCV('24), CVPR('23-'25), ICCV('23), NeurIPS(‚Äô24), ICLR(‚Äô25), ICML('25), 3DV(‚Äô24-‚Äô25), AAAI(‚Äô24), ACMMM(‚Äô21‚Äë‚Äô24), ACCV(‚Äô24), ISMAR(‚Äô23,‚Äô24), IEEEVR(‚Äô24)</li>
    <li>Journal Reviewer: TPAMI, IJCV, TIP, TMM, TCSVT, TOMM, TVCJ, Computers & Graphics, The Visual Computer</li>
    <li>He is a native speaker of <a href="https://en.wikipedia.org/wiki/Teochew_dialect">Teochew</a>, fluent in English, Mandarin, <a href="https://en.wikipedia.org/wiki/Cantonese">Cantonese</a>, and also familiar with <a href="https://en.wikipedia.org/wiki/Singlish">Singlish</a>.
    </li>
  </ul>
</div>


<div class="container">
  <script type='text/javascript' id='clustrmaps'
    src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=300&t=tt&d=rZHkm--x6O2bEyO0Je3uy1kjPw-mXX0YCKFUg287Tc0&co=ffffff&ct=808080&cmo=3acc3a&cmn=ff5353'></script>
</div>
